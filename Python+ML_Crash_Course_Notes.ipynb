{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python+ML Crash Course Notes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RN9XBfeBQJD9",
        "T16jU228Q3qG",
        "pcGUSTdDQGWX"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM8Ldiv8RMEEaITeo63mAxJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JetPlaneJJ/Python-ML-CrashCourse-Notes/blob/main/Python%2BML_Crash_Course_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKzGrIfcI0aj"
      },
      "source": [
        "# **(WIP!) This notebook contains: short exercise refreshers on python, image recognition sample using ImageAI's model training class, and notes on Google's ML crash course.**\n",
        "\n",
        "*Note: ImageAI does not support Tensorflow 2 right now.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAHa1bdpPlf4"
      },
      "source": [
        "# Python Refreshers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6jrQ1aqH_x4"
      },
      "source": [
        "# Dependencies \n",
        "# !pip install imageai\n",
        "# !pip install keras\n",
        "# !pip install numpy\n",
        "# !pip install opencv-python\n",
        "# !pip install tensorflow==1.13.1\n",
        "!pip show tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN9XBfeBQJD9"
      },
      "source": [
        "## Important built-in functions/types to remember\n",
        "- dir() â€“ lists out the objects attributes in current namespace.\n",
        "- int() - converts string or number to Int\n",
        "- input() - reads user input (prompt)\n",
        "- range(start,end,step) or range(end) - returns an object that produces a sequence of numbers.\n",
        "- isdigit() - checks digits in strings\n",
        "- pop() - remove list item\n",
        "- sorted() - sorts entire list\n",
        "- set() - mutable collection of unique items\n",
        "\n",
        "## Important syntax\n",
        "- elif = else if\n",
        "- and or is in\n",
        "- try --> except --> else --> finally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTdBJ6KHQkLD"
      },
      "source": [
        "# Short coding examples\n",
        "name = \"Alex\"\n",
        "print(name)\n",
        "name2 = 'Mary'\n",
        "print(name is name2)\n",
        "\n",
        "print('t' in 'Python')\n",
        "python_string = 'Python'\n",
        "python_string += '2.7'\n",
        "print(python_string)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T16jU228Q3qG"
      },
      "source": [
        "## Data Structures\n",
        "List and Tuple sequences are heterogeneous types (objects of diff types)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A-DOR7qQ3Mk"
      },
      "source": [
        "# tuples\n",
        "pos1 = (24,50)\n",
        "t1 = (1,'mango',2,'mango',3,'strawberry',4,'peach')\n",
        "print(t1.count('mango'))\n",
        "print(pos1[0])\n",
        "\n",
        "# lists (functionally same as arrays, except mutable!)\n",
        "cars = [\"Ford\", \"Volvo\", \"BMW\"]\n",
        "cars.append(\"Honda\")\n",
        "cars.remove(\"Ford\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcGUSTdDQGWX"
      },
      "source": [
        "## Classes, Magic Methods\n",
        "- \\__int\\__(self, â€¦) â€“ initializer method invoked during object creation.\n",
        "- \\__add\\__(self ,other) â€“ overloads object's addition\n",
        "- \\__eq\\__(self,other) - overloads equality comparison for object\n",
        "- all class members are public, no private/protected/friend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn1wX_MbTXA7",
        "outputId": "a9f6fe13-eddb-4d01-a863-e79c77b6da6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# classes\n",
        "class MyClass:\n",
        "  val = 0\n",
        "  def __init__(self, value):\n",
        "    self.val = value\n",
        "  def set(self, value):\n",
        "    self.val = value\n",
        "  def get(self, value):\n",
        "    return self.val \n",
        "\n",
        "c = MyClass(10)\n",
        "print(c.val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue5ZxwJCUg_u"
      },
      "source": [
        "## File Handling\n",
        "- file_handle = open(file_name, *access_mode*) --> reference to file object\n",
        "  - access_mode = how to read the file\n",
        "    - â€˜Uâ€™: all line terminators = new line\n",
        "    - â€˜+â€™ #read and write\n",
        "    - â€˜bâ€™ # binary\n",
        "    - â€˜tâ€™ # text mode\n",
        "  - can also access in combination: â€˜rbâ€™, â€˜rtâ€™, â€˜wbâ€™...\n",
        "\n",
        "- readline() - from file 1 line as 1 string\n",
        "- write() - write 1 string to file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABEKukkqc8tN"
      },
      "source": [
        "## NumPy and Pandas\n",
        "NumPy: makes creating arrays + linear algebra easier.\n",
        "Pandas: represent datasets in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov0kewt3dAjN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Create and populate a 5x2 NumPy array.\n",
        "my_data = np.array([[0, 3], [10, 7], [20, 9], [30, 14], [40, 15]])\n",
        "my_column_names = ['temperature', 'activity']\n",
        "my_dataframe = pd.DataFrame(data=my_data, columns=my_column_names)\n",
        "\n",
        "# Print the entire DataFrame (will show row numbers too)\n",
        "print(my_dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJnQ4xzIdUwJ"
      },
      "source": [
        "# Create a new column named adjusted.\n",
        "# Adds 2 from activity column cells\n",
        "my_dataframe[\"adjusted\"] = my_dataframe[\"activity\"] + 2 \n",
        "print(my_dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhORZHW9dr7t"
      },
      "source": [
        "# Isolating data in table\n",
        "print(\"Rows #0, #1, and #2:\")\n",
        "print(my_dataframe.head(3), '\\n')\n",
        "\n",
        "print(\"Row #2:\")\n",
        "print(my_dataframe.iloc[[2]], '\\n')\n",
        "\n",
        "print(\"Rows #1, #2, and #3:\")\n",
        "print(my_dataframe[1:4], '\\n')\n",
        "\n",
        "print(\"Column 'temperature':\")\n",
        "print(my_dataframe['temperature'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oSlaSmqhZNZ"
      },
      "source": [
        "# Slightly more complex stuff\n",
        "random_numbers = np.random.randint(low=0, high=101, size=(3,4))\n",
        "names = ['Eleanor', 'Chidi', 'Tahani', 'Jason']\n",
        "df = pd.DataFrame(data=random_numbers, columns=names)\n",
        "\n",
        "print(df)\n",
        "print(df['Eleanor'][0], '\\n')\n",
        "\n",
        "df[\"Janet\"] = df[\"Tahani\"] + df[\"Jason\"] \n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRiydhlVhlsP"
      },
      "source": [
        "# ASSIGNING DATAFRAME != COPYING!!!!!!!\n",
        "\n",
        "# Separate copy of my_dataframe\n",
        "copy_of_my_dataframe = my_dataframe.copy()\n",
        "\n",
        "# REFERENCE only!\n",
        "reference_to_df = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIQG503VV8pH"
      },
      "source": [
        "\n",
        "See here for the below notes content: https://developers.google.com/machine-learning/crash-course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68uzijnSWGxn"
      },
      "source": [
        "# Framing\n",
        "- **\"label\"** = thing we wanna see as result (y-variable, ex: future price of Nike shoes)\n",
        "  - labeled examples = specific instance of data with both feature + label\n",
        "  - unlabeled example = feature, but no label (\\{features, ?\\}: (x, ?))\n",
        "- **\"feature\"** = thing we input (eX: time of day email was sent)\n",
        "\n",
        "- **Training:** you \"teach\" model pictures that are already labeled cats and allow it to connect unspecified pics and the label cat.\n",
        "  - Inference = opposite of train, applying the trained model to unlabeled examples, like predicting median house value.\n",
        "- **Regression** = continous predictions\n",
        "- **Classification** = predict discrete values (ex: \"This animal is a cat\", \"This email is spam\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rane9h5OYbl8"
      },
      "source": [
        "# Linear Regression\n",
        "- **Empirical risk minimization**: find a model that minimizes loss after seeing many examples.\n",
        "- **Mean square error (MSE)** = avg sqrd loss per example over the whole dataset. In most cases, more accurate than using Total Loss.\n",
        "- Gradient descent --> you're on a slope (can be 2D, 3D, etc). The slope/grad is negative, so move in that direction (if minimizing).\n",
        "- **\"Learning Rate\" = Step Size.** If too small, takes forever (but accurate). Too big? Jumps around wildly/overshoots the min.\n",
        "  - \"Batch\" - how many examples you use to calculate gradient. \n",
        "  - **Stochastic Grad Desc** = Batch size 1 per iteration.\n",
        "  - Mini-batch Stochastic Grad Desc = between full-batch and SGD. ~10 -1,000 examples, random. \n",
        "    - Reduces amount of noise in SGD, efficiency > full-batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPzJxDRpPpQi"
      },
      "source": [
        "# Image Classification using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxZ0gPV-IVS0",
        "outputId": "a2b15237-439a-4fcc-f862-be6f38cd1502",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Credits: https://stackabuse.com/image-recognition-in-python-with-tensorflow-and-keras/\n",
        "\n",
        "# 1. import necessary libraries\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.constraints import maxnorm\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "# 2. for testing so you can replicate the same results\n",
        "seed = 21\n",
        "\n",
        "# 3. load the dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# 4. Normalize the data, divide by 255 (value of color, etc)\n",
        "X_train = X_train.astype('float32') # convert to floats from ints\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl8dCD8MwFuU"
      },
      "source": [
        "## One-hot encoding!\n",
        "- you might have categorical data. ex: \"dog\", \"blue\", \"taco\"\n",
        "- need to convert to NUMBERS for ML to work!\n",
        "- options:\n",
        "  - turn them into **Unique integers**. ex: \"dog\" = 1, \"cat\" = 2. Why is this not good enough in some cases?\n",
        "    - leads to half of a result (in between 2 values)\n",
        "    - maybe poor performance\n",
        "    - can't work on images (image shouldn't be 0.5 Rabbit 0.5 Shark)\n",
        "  - **ONE-HOT-ENCODING**: first, do the integer conversion, then a new binary variable is added for each unique integer value. Aka \"dummy variables\".\n",
        "    - ex: red = 1 0 0, green = 0 1 0, blue = 0 0 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3zw-IYFwFen"
      },
      "source": [
        "# 5. One-hot encode our outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "class_num = y_test.shape[1]\n",
        "\n",
        "# 6. Design the CNN (Convolutional Neural Network)\n",
        "model = Sequential() # given blueprint for building a model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCJPUda7ydBD"
      },
      "source": [
        "## Convolution\n",
        "\n",
        "See this video for more awesome detailed visual explanation! https://www.youtube.com/watch?v=ILsA4nyG7I0&ab_channel=BrandonRohrer\n",
        "\n",
        "**Convolution is how the input (image) gets modified by some filter. In CNNs, we have to apply many filters on the image so we can simplify/map the image and auto-identify different parts of it.**\n",
        "- You can do stuff like determine if an image contains a horizontal line after slowly scanning an image from left to right. \n",
        "- More examples: a matrix like [0 0 0; 1 1 1; 0 0 0] can be used as a convolution to detect horizontal lines (the higher value = darker pixel).\n",
        "\n",
        "Other cool vocab:\n",
        "- **Channels**: represents something about the image. Example would be 3 channels = RED, GREEN, BLUE\n",
        "  - assume scenario: 2D convolutions applied on images. Colored image data = matrix ð‘¤Ã—â„Žxc, or values of width of the image * height * color.\n",
        "    - the convol. layer receives the above, then spits out another (wxhxc), an \"activation map\" \n",
        "    - \"activation/feature map\" = image showing you the features detected in original image\n",
        "\n",
        "- **Filter/Kernel**: Filters are a 2d-array of weights (numbers) that are applied to the image. All images are arrays of numbers (representing each pixel).\n",
        "  - Imagine your real life camera capturing a full image (all pixels). Then you take a \"tiny camera\" (commonly 3x3 pixel large). \n",
        "  - Take a \"snapshot\" of a portion of the image (from top left to bottom right), producing another array that is the dot product of the weight and the filter-sized input.\n",
        "  - Then, depending on how you specify the \"stride\", you move your \"tiny camera\" over by however many pixels to the and do the same thing!\n",
        "\n",
        "- **Pooling** = is a layer extracting features from the image output of a convolution layer.\n",
        "  - Note: important to not include too many pooling layers! Each time discards some data.\n",
        "\n",
        "- **Neuron**\n",
        "  - brain context: cells receiving input from external world, outputs motor commands, relays electric signals to other nerve cells/neurons. needs to reach some certain input threshold to be \"fired\"/activated.\n",
        "  - ML context: like a math function (simply, input --> output)\n",
        "    - Ex1: **Sigmoid/\"squashing\"** (your outputs are bounded on the high/low sides, maybe like -1 to 1)\n",
        "    - Ex2: **ReLu** (Rectified Linear Activation Function) non-linear, values can get HECKA large, NO NEGATIVES so certain patterns may not be captured:(. Generally stable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjXeQq2dyZkx"
      },
      "source": [
        "# 7. Implement CNN first layer.\n",
        "# 32 = number of channels\n",
        "# 3x3 = size of filter \n",
        "# padding=same, we aren't changing the size of the image at all.\n",
        "model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))\n",
        "\n",
        "# relu = ReLu Neuron/Activation Function (way to help model learn patterns)\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# 8. Prevent overfitting (Dropout Layer)\n",
        "# randomly eliminate some of the connections between the layers \n",
        "# 0.2 -> drops 20% of the existing connections\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "# 9. Add Batch Norm\n",
        "# Ensures that network always creates activations with the same distribution.\n",
        "# Causes higher learning rates, faster and more stable neural networks.\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 10. Repeat layers to give network more representations to work off of.\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 11. Must flatten our data\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mYFIYWACp4Q"
      },
      "source": [
        "# 12. Create first densely connected layer.\n",
        "# 256 = # neurons in dense layer.\n",
        "model.add(Dense(256, kernel_constraint=maxnorm(3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "model.add(Dense(128, kernel_constraint=maxnorm(3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# 13. In the final layer, each neuron represents the distinct class. Ex: \"Dog\".\n",
        "# softmax = select neuron w/ highest probability \n",
        "# (voting that the image is indeed a Dog)\n",
        "model.add(Dense(class_num))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmT8YVg6DS7L",
        "outputId": "0fcfcde7-b134-4bb7-c07b-bd6ccb42af36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 14. Compile! And optimizers. And print results.\n",
        "epochs = 25\n",
        "optimizer = 'adam'\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer=optimizer, metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               8388864   \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 8,518,730\n",
            "Trainable params: 8,517,514\n",
            "Non-trainable params: 1,216\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zkaY0GKEKmT"
      },
      "source": [
        "## Actually Training the Model!\n",
        "- Just call the fit() function on model and pass in chosen paramters.\n",
        "- **Epoch**: # of passes of the entire training dataset the algorithm has completed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4lDW-EMESqF",
        "outputId": "8358a80a-b71e-48f9-fc26-fde832552066",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "numpy.random.seed(seed)\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, \n",
        "          batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "782/782 [==============================] - 525s 672ms/step - loss: 1.4127 - accuracy: 0.4975 - val_loss: 1.2792 - val_accuracy: 0.5396\n",
            "Epoch 2/25\n",
            "782/782 [==============================] - 525s 671ms/step - loss: 0.9540 - accuracy: 0.6641 - val_loss: 0.8557 - val_accuracy: 0.6955\n",
            "Epoch 3/25\n",
            "782/782 [==============================] - 515s 659ms/step - loss: 0.8245 - accuracy: 0.7104 - val_loss: 0.8632 - val_accuracy: 0.7005\n",
            "Epoch 4/25\n",
            "782/782 [==============================] - 521s 666ms/step - loss: 0.7819 - accuracy: 0.7257 - val_loss: 1.0391 - val_accuracy: 0.6378\n",
            "Epoch 5/25\n",
            "782/782 [==============================] - 522s 667ms/step - loss: 0.7527 - accuracy: 0.7350 - val_loss: 0.9971 - val_accuracy: 0.6601\n",
            "Epoch 6/25\n",
            "782/782 [==============================] - 521s 667ms/step - loss: 0.7267 - accuracy: 0.7440 - val_loss: 0.8173 - val_accuracy: 0.7172\n",
            "Epoch 7/25\n",
            "782/782 [==============================] - 526s 673ms/step - loss: 0.6996 - accuracy: 0.7546 - val_loss: 0.7374 - val_accuracy: 0.7402\n",
            "Epoch 8/25\n",
            "782/782 [==============================] - 536s 685ms/step - loss: 0.6866 - accuracy: 0.7616 - val_loss: 0.7414 - val_accuracy: 0.7332\n",
            "Epoch 9/25\n",
            "782/782 [==============================] - 541s 692ms/step - loss: 0.6588 - accuracy: 0.7706 - val_loss: 0.6816 - val_accuracy: 0.7585\n",
            "Epoch 10/25\n",
            "782/782 [==============================] - 542s 693ms/step - loss: 0.6500 - accuracy: 0.7749 - val_loss: 0.6392 - val_accuracy: 0.7768\n",
            "Epoch 11/25\n",
            "782/782 [==============================] - 534s 683ms/step - loss: 0.6349 - accuracy: 0.7793 - val_loss: 0.6609 - val_accuracy: 0.7698\n",
            "Epoch 12/25\n",
            "782/782 [==============================] - 541s 692ms/step - loss: 0.6308 - accuracy: 0.7799 - val_loss: 0.7251 - val_accuracy: 0.7479\n",
            "Epoch 13/25\n",
            "782/782 [==============================] - 535s 684ms/step - loss: 0.6115 - accuracy: 0.7872 - val_loss: 0.6632 - val_accuracy: 0.7706\n",
            "Epoch 14/25\n",
            "782/782 [==============================] - 533s 681ms/step - loss: 0.6061 - accuracy: 0.7897 - val_loss: 0.5979 - val_accuracy: 0.7944\n",
            "Epoch 15/25\n",
            "782/782 [==============================] - 524s 670ms/step - loss: 0.5941 - accuracy: 0.7923 - val_loss: 0.6315 - val_accuracy: 0.7826\n",
            "Epoch 16/25\n",
            "782/782 [==============================] - 532s 681ms/step - loss: 0.5897 - accuracy: 0.7963 - val_loss: 0.6580 - val_accuracy: 0.7674\n",
            "Epoch 17/25\n",
            "782/782 [==============================] - 541s 692ms/step - loss: 0.5883 - accuracy: 0.7955 - val_loss: 0.6074 - val_accuracy: 0.7935\n",
            "Epoch 18/25\n",
            "721/782 [==========================>...] - ETA: 40s - loss: 0.5743 - accuracy: 0.8002"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5WyiTXEEqae"
      },
      "source": [
        "# How well did our model categorize the data?\n",
        "# (The data we were given had images already linked to the answers, \n",
        "# what they were. Dogs? Cats? Person...?)\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}